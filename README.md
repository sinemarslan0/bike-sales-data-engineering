## Bike Sales Data Engineering

This repository demonstrates an end-to-end ETL (Extract–Transform–Load) pipeline, basic data-warehouse modeling in SQL Server, and preparation for visualization on the Kaggle Bike Sales dataset. The work is implemented with Jupyter Notebooks, and the cleaned data is loaded into Microsoft SQL Server.

### Contents and Folder Structure
- `01_eda_bike_sales.ipynb`: Exploratory Data Analysis (EDA), ETL Step 1 – inspecting the raw data
- `02_load_transform_bike_sales.ipynb`: Transformations, ETL Step 2 – cleaning/normalization and producing `data/cleaned/` outputs
- `03_load_to_sqlserver.ipynb`: ETL Step 3 – documenting the process to load the cleaned data into SQL Server
- `data/`: Raw CSV files
- `data/cleaned/`: Cleaned CSVs generated by Notebook 2
- `erd.png`, `erd_cleaned.png`: Entity–Relationship Diagrams (raw vs. cleaned schema views)

### Setup
A standard Python 3.11+ environment is sufficient. Notebooks use: `pandas`, `numpy`, `matplotlib`, `seaborn`, `missingno`, `jupyter`.

Example environment setup:
```bash
python -m venv .venv
.venv\Scripts\activate  # Windows PowerShell
pip install --upgrade pip
pip install jupyter pandas numpy matplotlib seaborn missingno
jupyter lab  # or jupyter notebook
```

Note: For the load step, you need a local SQL Server and SSMS (SQL Server Management Studio).

---

## ETL Pipeline Overview

1) Extract – `01_eda_bike_sales.ipynb`
- Load raw CSVs from `data/`: `Addresses.csv`, `BusinessPartners.csv`, `Employees.csv`, `ProductCategories.csv`, `Products.csv`, etc.
- Inspect table size, column types, and missingness (`.shape`, `.info()`, `missingno.matrix`).
- Identify data quality issues: BOM-related column names, placeholder date fields, type inconsistencies, empty/uninformative columns.

2) Transform – `02_load_transform_bike_sales.ipynb`
- Apply cleaning and normalization to all tables. Highlights:
  - `addresses`: Rename BOM-based `ï»¿ADDRESSID` → `ADDRESSID`; drop placeholder validity date fields; schema/type checks; save to `data/cleaned/addresses.csv`.
  - `business_partners`: Drop columns with excessive missingness or low value (e.g., `FAXNUMBER`); convert `CREATEDAT`/`CHANGEDAT` from `YYYYMMDD` to datetime; set `PHONENUMBER` to string; save to `data/cleaned/business_partners.csv`.
  - `employees`: Drop irrelevant columns; remove placeholder validity date fields; schema/type checks; save to `data/cleaned/employees.csv`.
  - `product_categories`: Convert `CREATEDAT` to datetime; save to `data/cleaned/product_categories.csv`.
  - `products`: Remove empty/non-informative columns; perform type fixes and schema checks; save cleaned outputs (e.g., `data/cleaned/products.csv`).
- Validate each table with quick `.info()` checks.

3) Load – `03_load_to_sqlserver.ipynb`
- Create the `BikeSalesDW` database in SSMS.
- Define tables with `CREATE TABLE` statements (dimension/fact-like structures where applicable).
- Add foreign keys to enforce referential integrity.
- Use SSMS “Import Flat File” wizard to import every cleaned CSV from `data/cleaned/`.
- Run validation queries to confirm row counts, null handling, and relationship integrity.

---

## Notebooks Summary

### 01 – Exploratory Data Analysis (`01_eda_bike_sales.ipynb`)
- Goal: Understand raw data quality, identify missingness and type inconsistencies, and derive transformation requirements.
- Highlights:
  - Load all CSVs and generate quick summaries (`.shape`, `.info()`).
  - Visualize missingness with `missingno`.
  - Check uniqueness and previews for key domains like country, category, and product (`.head()`, `value_counts()`).

### 02 – Transformation and Cleaning (`02_load_transform_bike_sales.ipynb`)
- Goal: Produce a consistent, analysis-ready schema with correct types and without redundant columns.
- Highlights:
  - Fix BOM-induced column names.
  - Remove placeholder date fields or convert to proper datetime.
  - Drop empty/duplicate/non-informative columns.
  - Persist cleaned outputs to `data/cleaned/` per table.

### 03 – Load into SQL Server (`03_load_to_sqlserver.ipynb`)
- Goal: Host cleaned data in SQL Server with enforced referential integrity.
- Highlights:
  - Create the `BikeSalesDW` database and define table schemas.
  - Establish foreign key relationships.
  - Import CSVs via SSMS “Import Flat File” and run validation checks.

---

## ERD and Schema
- `erd.png`: Relationship diagram for the raw dataset.
- `erd_cleaned.png`: Cleaned schema diagram as loaded into SQL Server.

The ERDs summarize core entities (e.g., addresses, products, categories, business partners, employees) and their relationships. Foreign keys enforce data integrity.

---

## How to Run
1. Set up the Python environment and launch Jupyter.
2. Open and run `01_eda_bike_sales.ipynb` to assess raw data quality.
3. Run `02_load_transform_bike_sales.ipynb` to produce cleaned CSVs in `data/cleaned/`.
4. Install/configure SQL Server and SSMS, then follow `03_load_to_sqlserver.ipynb` to create the database and import the cleaned CSVs.

---

## Notes and Improvement Ideas
- Automate transformations via a Python CLI or an orchestrator (e.g., Airflow, Prefect).
- Add data quality tests.
